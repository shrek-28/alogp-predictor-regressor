{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f1ffbcd-b4b9-45eb-b965-d331c7681e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374abc2e-45ac-443d-a5f7-29a1bb419d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48492 entries, 0 to 48491\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Coconut_ID           48492 non-null  object \n",
      " 1   Molecular_Formula    48492 non-null  object \n",
      " 2   Molecular_Weight     48492 non-null  float64\n",
      " 3   Number_of_Nitrogens  48492 non-null  int64  \n",
      " 4   Number_of_Oxygens    48492 non-null  int64  \n",
      " 5   Number_of_Carbons    48492 non-null  int64  \n",
      " 6   Total_Atom_Number    48492 non-null  int64  \n",
      " 7   Bond_Count           48492 non-null  int64  \n",
      " 8   ALogP                48492 non-null  float64\n",
      " 9   APolarSurfaceArea    48492 non-null  float64\n",
      " 10  Topo_PSA             48492 non-null  float64\n",
      "dtypes: float64(4), int64(5), object(2)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'cleaned_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851922fb-c489-4774-ab00-6a63f154684e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coconut_ID</th>\n",
       "      <th>Molecular_Formula</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Number_of_Nitrogens</th>\n",
       "      <th>Number_of_Oxygens</th>\n",
       "      <th>Number_of_Carbons</th>\n",
       "      <th>Total_Atom_Number</th>\n",
       "      <th>Bond_Count</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>APolarSurfaceArea</th>\n",
       "      <th>Topo_PSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNP0000002</td>\n",
       "      <td>C27H36N2O15S</td>\n",
       "      <td>660.651</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>81</td>\n",
       "      <td>47</td>\n",
       "      <td>-2.08210</td>\n",
       "      <td>260.03</td>\n",
       "      <td>260.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNP0000003</td>\n",
       "      <td>C34H30O10</td>\n",
       "      <td>598.604</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>74</td>\n",
       "      <td>50</td>\n",
       "      <td>3.63422</td>\n",
       "      <td>137.82</td>\n",
       "      <td>137.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNP0000004</td>\n",
       "      <td>C32H26O9</td>\n",
       "      <td>554.551</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>3.32262</td>\n",
       "      <td>139.59</td>\n",
       "      <td>139.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNP0000005</td>\n",
       "      <td>C33H42O6</td>\n",
       "      <td>534.693</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "      <td>6.87940</td>\n",
       "      <td>78.90</td>\n",
       "      <td>78.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNP0000006</td>\n",
       "      <td>C31H24O9</td>\n",
       "      <td>540.524</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>3.01962</td>\n",
       "      <td>150.59</td>\n",
       "      <td>150.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coconut_ID Molecular_Formula  Molecular_Weight  Number_of_Nitrogens  \\\n",
       "0  CNP0000002      C27H36N2O15S           660.651                    2   \n",
       "1  CNP0000003         C34H30O10           598.604                    0   \n",
       "2  CNP0000004          C32H26O9           554.551                    0   \n",
       "3  CNP0000005          C33H42O6           534.693                    0   \n",
       "4  CNP0000006          C31H24O9           540.524                    0   \n",
       "\n",
       "   Number_of_Oxygens  Number_of_Carbons  Total_Atom_Number  Bond_Count  \\\n",
       "0                 15                 27                 81          47   \n",
       "1                 10                 34                 74          50   \n",
       "2                  9                 32                 67          47   \n",
       "3                  6                 33                 81          42   \n",
       "4                  9                 31                 64          46   \n",
       "\n",
       "     ALogP  APolarSurfaceArea  Topo_PSA  \n",
       "0 -2.08210             260.03    260.03  \n",
       "1  3.63422             137.82    137.82  \n",
       "2  3.32262             139.59    139.59  \n",
       "3  6.87940              78.90     78.90  \n",
       "4  3.01962             150.59    150.59  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c27f2ee-ee07-46c5-9e13-cdb5ad0222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Coconut_ID', 'Molecular_Formula', 'ALogP'])  # Drop non-informative columns and target\n",
    "y = data['ALogP']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e40a13-5ddc-4729-8a95-24689b1e1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessor with placeholder scaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), X.columns)  # Scaler will be dynamically selected\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Placeholder preprocessor\n",
    "    ('model', RandomForestRegressor(random_state=42))  # RandomForestRegressor as the model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e448d9-b175-4098-8819-7e4b118ad793",
   "metadata": {},
   "source": [
    "## What is a Pipeline in Machine Learning?\n",
    "\n",
    "In machine learning, a pipeline is a way of organizing the steps in a workflow so that they can be applied in sequence. It ensures that each step, like preparing the data or applying a model, happens automatically and in the correct order. You can think of a pipeline like an assembly line where each station processes the product before it moves to the next station.\n",
    "\n",
    "## What’s Happening in This Code?\n",
    "\n",
    "### Preprocessor:\n",
    "\n",
    "The preprocessor is like the \"clean-up crew\" that prepares the data before it goes into the machine learning model.\n",
    "\n",
    "Here, the preprocessor is using something called a ColumnTransformer. It applies a transformation to the data columns. In this case, it is using a scaler (specifically StandardScaler) to scale the data. Scaling means adjusting the data so it has a mean of 0 and a standard deviation of 1. This is important because some models (like RandomForestRegressor) work better when the data is on a similar scale.\n",
    "\n",
    "The StandardScaler() will be applied to all the columns of the data (X.columns), which is dynamically chosen based on the dataset you use.\n",
    "\n",
    "### Pipeline:\n",
    "\n",
    "A pipeline is created with two main steps:\n",
    "\n",
    "Preprocessing: First, the data will go through the preprocessor (where the scaling happens).\n",
    "\n",
    "Model: After preprocessing, the processed data will go into a RandomForestRegressor, which is a type of machine learning model used for predicting continuous values (like predicting prices or quantities). The random_state=42 ensures that the model gives consistent results when run multiple times (it helps with reproducibility).\n",
    "\n",
    "## Why Do We Use This?\n",
    "Preprocessing ensures that the data is ready for the machine learning model. Raw data often needs cleaning and scaling to help the model perform better.\n",
    "The pipeline ties the whole process together, so that data is processed and then fed into the model in one go. This makes the code more efficient, easier to maintain, and helps avoid errors in processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9973509-b2ed-4edc-8d47-ea8a90d679f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "9 fits failed out of a total of 216.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 489, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 192, in _parallel_build_trees\n",
      "    tree._fit(\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"_tree.pyx\", line 172, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 287, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 942, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"_tree.pyx\", line 910, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"_utils.pyx\", line 35, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 2097152 bytes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 489, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 192, in _parallel_build_trees\n",
      "    tree._fit(\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"_tree.pyx\", line 172, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 287, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 942, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"_tree.pyx\", line 910, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"_utils.pyx\", line 35, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 1048576 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.84075616 -0.84396838         nan         nan         nan         nan\n",
      " -0.85162697 -0.85236235         nan -0.84662924 -0.84338369 -0.84333922\n",
      " -0.8576358  -0.85826757 -0.85296753 -0.85376069 -0.84939577 -0.8499632\n",
      " -0.86220358 -0.86353943 -0.85737739 -0.8580413  -0.85401651 -0.85497699\n",
      " -1.17324149 -1.17363291 -1.1759156  -1.17638331 -1.17545197 -1.17601604\n",
      " -1.17859507 -1.17948232 -1.17935716 -1.17997765 -1.17854833 -1.17936258\n",
      " -1.17694118 -1.17845479 -1.17811527 -1.17903807 -1.1776589  -1.17855228\n",
      " -1.17799111 -1.17915471 -1.17990164 -1.18066053 -1.17918398 -1.18019985\n",
      " -0.84343893 -0.84411692 -0.83958099 -0.84015252 -0.8365419  -0.83737404\n",
      " -0.8517448  -0.85445743 -0.84776892 -0.84914914 -0.84479989 -0.84598449\n",
      " -0.86174616 -0.86292379 -0.85772065 -0.85861769 -0.8529592  -0.8539842\n",
      " -0.86289425 -0.86471476 -0.85926078 -0.85993964 -0.85547392 -0.85643376]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__max_depth': 20, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 150, 'preprocessor__scaler': StandardScaler()}\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    # Scaler selection\n",
    "    'preprocessor__scaler': [StandardScaler(), MinMaxScaler()],\n",
    "    \n",
    "    # RandomForest parameters\n",
    "    'model__n_estimators': [50, 100, 150],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a13f56-be3a-4a7c-ac3d-c10dcadf0ce8",
   "metadata": {},
   "source": [
    "## What is Grid Search?\n",
    "\n",
    "1. Grid Search is a method for trying different combinations of parameters (settings) for a machine learning model. The goal is to find the best combination that results in the most accurate model.\n",
    "2. Think of it like trying different recipes for a cake, where each recipe represents a different combination of ingredients (parameters) to see which one makes the best cake (model).\n",
    "\n",
    "## Defining the Parameter Grid (param_grid)\n",
    "The parameter grid defines all the combinations of parameters that we want to test. Here’s how it’s structured:\n",
    "\n",
    "Scaler Selection: The first setting we want to test is the type of scaling we apply to the data. We can either use:\n",
    "\n",
    "StandardScaler: Scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "MinMaxScaler: Scales the data to a range between 0 and 1.\n",
    "\n",
    "Random Forest Parameters: These are the settings for the RandomForestRegressor model that we are using to make predictions:\n",
    "\n",
    "n_estimators: The number of trees in the random forest. We are testing with 50, 100, and 150 trees.\n",
    "\n",
    "max_depth: The maximum depth (or levels) that each tree can have. It controls how deep each tree can grow. We test with None (no limit), 10, and 20.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split a node into two. We are testing values of 2 and 5.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. We test values of 1 and 2.\n",
    "\n",
    "## Setting Up GridSearchCV\n",
    "GridSearchCV is used to perform the grid search. It takes in the pipeline (which includes preprocessing and the model), the parameter grid we defined above, and some additional settings:\n",
    "\n",
    "scoring: We use neg_mean_squared_error to evaluate the model’s performance based on the error in predictions (lower is better).\n",
    "\n",
    "cv=3: This means the data will be split into 3 parts (cross-validation) to check how well the model performs on different data splits.\n",
    "\n",
    "n_jobs=-1: This tells GridSearchCV to use all available processors to speed up the process.\n",
    "\n",
    "verbose=2: This option provides more detailed information about what’s happening during the grid search.\n",
    "\n",
    "## Running the Grid Search\n",
    "\n",
    "grid_search.fit(X_train, y_train): This command starts the grid search. It tests all the different combinations of parameters and evaluates how well each one works using the training data (X_train and y_train).\n",
    "\n",
    "## Getting the Best Model and Parameters\n",
    "\n",
    "best_params = grid_search.best_params_: This gives us the best combination of settings (parameters) that resulted in the best performance.\n",
    "\n",
    "best_model = grid_search.best_estimator_: This gives us the best model that was found during the grid search, with all the optimal settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d807e-253d-4dcd-a2b2-c7d636a6e751",
   "metadata": {},
   "source": [
    "## The best parameters found by the grid search are:\n",
    "\n",
    "### model__max_depth: 20:\n",
    "\n",
    "The optimal maximum depth for the trees in the Random Forest model is 20. This means the trees are allowed to grow up to 20 levels deep, which helps in capturing complex patterns in the data without being too shallow (which could miss important information) or too deep (which could lead to overfitting).\n",
    "\n",
    "### model__min_samples_leaf: 1:\n",
    "\n",
    "The model performs best when there is no restriction on the minimum number of samples required at each leaf node (i.e., each leaf can contain just one sample). This allows the trees to capture finer details in the data, though it might also increase the risk of overfitting, especially if the dataset is noisy.\n",
    "\n",
    "### model__min_samples_split: 2:\n",
    "\n",
    "The optimal value for minimum samples required to split a node is 2, meaning the model can split a node even if it only has 2 samples. This enables the model to make more splits, potentially capturing more granular patterns in the data, but can also lead to overfitting if the tree becomes too specific to the training data.\n",
    "\n",
    "### model__n_estimators: 150:\n",
    "\n",
    "The model works best when there are 150 trees in the forest. More trees generally improve the model’s performance, but too many can make the model more computationally expensive. In this case, 150 trees provide a good balance between performance and computational efficiency.\n",
    "\n",
    "### preprocessor__scaler: StandardScaler():\n",
    "\n",
    "The StandardScaler is the optimal choice for scaling the data. This scaler standardizes the features to have a mean of 0 and a standard deviation of 1, which helps improve the performance of many machine learning models, especially when features have different units or ranges.\n",
    "\n",
    "## Overall Inference:\n",
    "The Random Forest model with 150 trees, a maximum depth of 20, and the ability to make splits with as few as 2 samples at each node will likely provide a good balance between capturing complex patterns and avoiding overfitting.\n",
    "\n",
    "Standard scaling of the data is also beneficial, which means the model benefits from data where features are adjusted to a common scale.\n",
    "\n",
    "The combination of these settings suggests the model is ready to generalize well to unseen data, with enough complexity to model the relationships in the data without overfitting too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8daf3079-059a-4922-bc3f-4c412920fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.5318305749217103\n",
      "Mean Squared Error (MSE): 0.7095168646858466\n",
      "Root Mean Squared Error (RMSE): 0.842328240465584\n",
      "R-squared (R²): 0.9346997669185286\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44439edb-dcb1-4d30-90bf-b9cb5c47c355",
   "metadata": {},
   "source": [
    "## Inference from the Evaluation Metrics:\n",
    "\n",
    "### R-squared (R²) = 0.9347:\n",
    "\n",
    "The model explains 93.47% of the variance in ALogP, which is a strong result. It indicates that the model fits the data well and is able to predict ALogP with high accuracy.\n",
    "\n",
    "### Mean Absolute Error (MAE) = 0.5318:\n",
    "\n",
    "The average error per prediction is about 0.53. Given that ALogP has a range of 70+, this error seems reasonable, suggesting that the model is performing well in terms of predicting the target values.\n",
    "\n",
    "### Mean Squared Error (MSE) = 0.7095:\n",
    "\n",
    "The MSE is slightly higher than MAE due to the squaring of errors, which penalizes larger deviations more. The value of 0.7095 is consistent with the high R² score, suggesting that large errors are not prevalent in the model’s predictions.\n",
    "\n",
    "### Root Mean Squared Error (RMSE) = 0.8423:\n",
    "\n",
    "The RMSE of 0.84 shows that the model has a relatively low average error per prediction, considering the scale of ALogP. It indicates that the model is making fairly accurate predictions on average.\n",
    "\n",
    "## Overall Inference:\n",
    "The model demonstrates strong performance with high accuracy (R² of 93.47%) and low prediction errors (MAE = 0.53, RMSE = 0.84). Given the range of ALogP, these errors are acceptable and suggest the model is reliable for predicting ALogP values. The model generalizes well to unseen data and is performing efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
